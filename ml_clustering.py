# -*- coding: utf-8 -*-
"""Final_Ml_clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IlfmsV-aiehxUUAmb7pQ9bBwxKKF-jOs
"""

import matplotlib.pyplot as plt
import seaborn as sns

import pandas as pd

data = pd.read_csv("/content/drive/MyDrive/Boun_rate_2001.csv")
print(data.describe())

data.shape

print(data.head())

print(data.info())

print(data.columns.tolist())

print(data.isnull().sum())

print(data.isnull().any())

print(data.isnull().sum().sum())

data['Traffic Source'].unique()

data['Traffic Source'].value_counts().plot(kind = 'pie', autopct = '%.1f')
plt.ylabel(None)
plt.title('Traffic Source')
plt.show()

sns.histplot(data['Page Views'], bins = 15, kde = True)
plt.title('Distribution of page Views')

plt.figure(figsize=(8,4))
sns.scatterplot(x='Time on Page', y='Conversion Rate', data=data)
plt.title('Time On Page vs Conversion Rate')
plt.xlabel('Time On Page')
plt.ylabel('Conversion Rate (in percentage)')
plt.show()

# creating a scatter plot
plt.figure(figsize=(10, 6))
sns.scatterplot(x='Session Duration', y='Bounce Rate', data=data)
plt.title('Session Duration vs Bounce Rate')
plt.xlabel('Session Duration (in minutes)')
plt.ylabel('Bounce Rate (in percentage)')
plt.show()

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
data['Traffic Source'] = le.fit_transform(data['Traffic Source'])
print(data.head())

data['Traffic Source'].unique()

data['Conversion Rate'].unique()

data_num = data.drop('Traffic Source', axis=1)
fig, axs = plt.subplots(len(data_num.columns), 1, dpi=95, figsize=(7,17)) # Change 6 to len(data_num.columns)
i = 0
for col in data_num.columns:
    axs[i].boxplot(data_num[col], vert=False)
    axs[i].set_ylabel(col)
    i+=1
plt.show()

from scipy.stats import zscore
numerical_columns = ['Bounce Rate']

z_scores = data[numerical_columns].apply(zscore)

outliers = (z_scores.abs() > 3)  # True for outliers
print("Outliers detected:")
print(outliers.sum())

from scipy.stats.mstats import winsorize

data['Bounce Rate'] = winsorize(data["Bounce Rate"], limits=[0.05, 0.05])
print(data.head())

from scipy.stats import zscore
numerical_columns = ['Bounce Rate']

z_scores = data[numerical_columns].apply(zscore)

outliers = (z_scores.abs() > 3)  # True for outliers
print("Outliers detected:")
print(outliers.sum())

import matplotlib.pyplot as plt
import numpy as np
from sklearn.cluster import DBSCAN
from sklearn import metrics
from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
from sklearn import datasets

# Select features for clustering (you might need to adjust this based on your dataset)
features = ['Page Views', 'Session Duration', 'Bounce Rate', 'Time on Page', 'Previous Visits', 'Conversion Rate']
X = data[features]

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

pca = PCA(n_components=2)  # Reduce to 2 dimensions if desired
X_scaled = pca.fit_transform(X_scaled)

# Create a GaussianMixture object
gmm = GaussianMixture(n_components=3, random_state=42)  # Adjust n_components

# Fit the model to the data (use X_reduced if PCA is applied)
gmm.fit(X_scaled)

# Get cluster labels
labels = gmm.predict(X_scaled)

# Add cluster labels to the original DataFrame
data['Cluster'] = labels


# Explore the characteristics of each cluster
for cluster in set(labels):
    print(f"Cluster {cluster}:")
    print(data[data['Cluster'] == cluster][features].describe())
    print("\n")

# Analyze the number of points in each cluster
print(data['Cluster'].value_counts())

# Visualize the clusters in 2D (if PCA is applied)
if 'X_scaled' in locals():  # Check if PCA was used
    plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels, cmap='viridis')
    plt.title('GMM Clustering (PCA-reduced Data)')
    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.show()
else:
    print("Visualization not possible for high-dimensional data without PCA.")

scaler = StandardScaler()
data_scaled = scaler.fit_transform(X_scaled)

# DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=2)
clusters = dbscan.fit_predict(data_scaled)

# Plotting the clusters
plt.figure(figsize=(8, 6))

# Plot each point and color by the cluster label
plt.scatter(data_scaled[:, 0], data_scaled[:, 1], c=clusters, cmap='viridis', s=100)

# Mark core samples with larger points
core_samples = dbscan.core_sample_indices_
plt.scatter(data_scaled[core_samples, 0], data_scaled[core_samples, 1])

from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=4, random_state=42)
clusters = kmeans.fit_predict(X_scaled)


# Step 5: Visualize the clusters in PCA space
plt.figure(figsize=(8, 6))
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap='viridis', s=100)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='X', s=200, label="Centroids")
plt.title("K-Means Clustering after PCA")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.legend()
plt.show()

from sklearn.cluster import KMeans
import pandas as pd

kmeans = KMeans(n_clusters=4, random_state=42)
clusters = kmeans.fit_predict(X_scaled)

# Add cluster labels to the original DataFrame
data['Cluster'] = clusters

# Get unique cluster labels
unique_clusters = data['Cluster'].unique()

# Describe each cluster and print total count
for cluster in unique_clusters:
    print(f"Cluster {cluster}:")
    cluster_data = data[data['Cluster'] == cluster]
    description = cluster_data[features].describe()
    print(description)

    # Print total count for the cluster
    total_count = len(cluster_data)
    print(f"Total Count: {total_count}")

    print("\n")

#  Visualize the clusters in PCA space
plt.figure(figsize=(8, 6))
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap='viridis', s=100)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='X', s=200, label="Centroids")
plt.title("K-Means Clustering after PCA")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.legend()
plt.show()

# prompt: isolation forest kmeans for this data

import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest

# Assuming X_scaled is your scaled data after PCA
# ... (Your existing code for data loading, scaling, and PCA)

# Isolation Forest
isolation_forest = IsolationForest(contamination='auto', random_state=42)
outlier_predictions = isolation_forest.fit_predict(X_scaled)

# Add Isolation Forest predictions to the DataFrame
data['IsolationForest_Outlier'] = outlier_predictions

# Analyze the number of outliers
print(data['IsolationForest_Outlier'].value_counts())

# Visualize the outliers in 2D (if PCA is applied)
if 'X_scaled' in locals():  # Check if PCA was used
    plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=outlier_predictions, cmap='viridis')
    plt.title('Isolation Forest Outlier Detection (PCA-reduced Data)')
    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.show()
else:
    print("Visualization not possible for high-dimensional data without PCA.")
# Isolation Forest
isolation_forest = IsolationForest(contamination='auto', random_state=42)
outlier_predictions = isolation_forest.fit_predict(X_scaled)

# Filter out outliers: Keep only inliers (data points with prediction == 1)
X_scaled_no_outliers = X_scaled[outlier_predictions == 1]

# Apply K-means on the data without outliers
kmeans = KMeans(n_clusters=4, random_state=42)
clusters = kmeans.fit_predict(X_scaled_no_outliers)
# Add cluster labels to the original DataFrame (adjust indexing if needed)
data['Cluster'] = -1  # Initialize with a temporary value
data.loc[outlier_predictions == 1, 'Cluster'] = clusters

# Visualize the clusters in PCA space
plt.figure(figsize=(8, 6))
plt.scatter(X_scaled_no_outliers[:, 0], X_scaled_no_outliers[:, 1],
            c=clusters, cmap='viridis', s=100)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
            c='red', marker='X', s=200, label="Centroids")
plt.title("K-Means Clustering after Outlier Removal (PCA)")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.legend()
plt.show()

# Explore the characteristics of each cluster
for cluster in set(clusters):  # Note: Using 'clusters' here, not 'labels'
    print(f"Cluster {cluster}:")
    cluster_data = data[data['Cluster'] == cluster]  # Using the 'Cluster' column
    description = cluster_data[features].describe()
    print(description)
    print(f"Total Count: {len(cluster_data)}")
    print("\n")

# Use a palette with 4 colors
palette = sns.color_palette("tab10", n_colors=4)  # Or any other palette with 4 or fewer colors

# Create the pairplot
sns.pairplot(data[features + ['Cluster']], hue='Cluster', diag_kind='kde', palette=palette)
plt.show()

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier()
rf.fit(X_scaled, clusters)  # X_scaled is your standardized data

# Get feature names after PCA (assuming 2 components)
pca_features = [f"Principal Component {i+1}" for i in range(X_scaled.shape[1])]

# Create Series with correct index
feature_importances = pd.Series(rf.feature_importances_, index=pca_features)

print(feature_importances.sort_values(ascending=False))

#Bar_graph
plt.figure(figsize=(10, 6))
sns.barplot(x=feature_importances.values, y=feature_importances.index)
plt.title("Feature Importances in Random Forest")
plt.xlabel("Importance Score")
plt.ylabel("Features")
plt.show()

# Select top features and visualize (adjust if needed)
top_features = feature_importances.nlargest(2).index
sns.scatterplot(x=X_scaled[:, 0], y=X_scaled[:, 1], hue=data['Cluster'])  # Use X_scaled for plotting
plt.xlabel(top_features[0])
plt.ylabel(top_features[1])
plt.show()

pca_3d = PCA(n_components=3)  # Apply PCA for 3 components
X_scaled_3d = pca_3d.fit_transform(scaler.fit_transform(X))  # Fit and transform

fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')

# Use X_scaled_3d for plotting
ax.scatter(X_scaled_3d[:, 0], X_scaled_3d[:, 1], X_scaled_3d[:, 2], c=clusters, cmap='viridis', s=50)

ax.set_title("3D Scatter Plot of Clusters")
ax.set_xlabel("Principal Component 1")
ax.set_ylabel("Principal Component 2")
ax.set_zlabel("Principal Component 3")
plt.show()

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score

# Using the existing cluster labels as targets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, clusters, test_size=0.3, random_state=42)

# Train a Random Forest Classifier
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)

# Predict on test set
y_pred = rf_model.predict(X_test)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

# Feature importance
importances = rf_model.feature_importances_
plt.figure(figsize=(10, 6))
sns.barplot(x=importances, y=pca_features)
plt.title("Feature Importance")
plt.show()

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score

# Train a Logistic Regression model
logistic_model = LogisticRegression(max_iter=1000, random_state=42)
logistic_model.fit(X_train, y_train)

# Predict on test set
y_pred_logistic = logistic_model.predict(X_test)

# Evaluate the model
print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_logistic))
print("Classification Report (Logistic Regression):\n", classification_report(y_test, y_pred_logistic))

from xgboost import XGBClassifier
from sklearn.metrics import classification_report, accuracy_score

# Train a Gradient Boosting model
xgb_model = XGBClassifier(random_state=42, eval_metric='logloss')
xgb_model.fit(X_train, y_train)

# Predict on test set
y_pred_xgb = xgb_model.predict(X_test)

# Evaluate the model
print("XGBoost Accuracy:", accuracy_score(y_test, y_pred_xgb))
print("Classification Report (XGBoost):\n", classification_report(y_test, y_pred_xgb))

from sklearn.svm import SVC

# Train an SVM model
svm_model = SVC(kernel='linear', random_state=42)
svm_model.fit(X_train, y_train)

# Predict on test set
y_pred_svm = svm_model.predict(X_test)

# Evaluate the model
print("SVM Accuracy:", accuracy_score(y_test, y_pred_svm))
print("Classification Report (SVM):\n", classification_report(y_test, y_pred_svm))

data.describe()